{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "import os\n",
    "import datetime\n",
    "from sklearn.decomposition import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/admin/Documents/Big data/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.chdir('/Users/admin/Documents/Big data/data')\n",
    "\n",
    "with open('Tickers.txt') as tickers:\n",
    "    reader=tickers.read().split(\"\\n\")\n",
    "    list_tickers=[read for read in reader]\n",
    "\n",
    "#data=pd.read_csv('AAPL.txt'.format(ticker), sep=\",\")\n",
    "data=pd.read_csv('AAPL.txt', sep=\",\")\n",
    "data['DATE']=data['DATE'].apply(lambda x : str(x))\n",
    "data['DATE']=data['DATE'].apply(lambda x : datetime.datetime.strptime(x,'%Y%m%d'))\n",
    "\n",
    "data.index=data[\"DATE\"]\n",
    "\n",
    "\n",
    "data=data[\" OPEN\"]\n",
    "data_df=pd.DataFrame(data)\n",
    "\n",
    "print(type(data_df))\n",
    "damaged_stocks = []\n",
    "nondamaged_stocks = ['AAPL']\n",
    "for ticker in list_tickers[2:]:        \n",
    "    df = pd.read_csv('{}.txt'.format(ticker.strip()), sep=\",\")\n",
    "    df['DATE']=df['DATE'].apply(lambda x : str(x))\n",
    "    df['DATE']=df['DATE'].apply(lambda x : datetime.datetime.strptime(x,'%Y%m%d'))\n",
    "    df.index=df[\"DATE\"]\n",
    "    df = pd.DataFrame(df[\" OPEN\"])\n",
    "    if len(df)<3000:\n",
    "        #print(ticker)\n",
    "        damaged_stocks.append(ticker.strip())        \n",
    "    else :\n",
    "        #print(type(df))\n",
    "        data_df=data_df.merge(df,left_index=True,right_index=True)\n",
    "        nondamaged_stocks.append(ticker.strip())  \n",
    "    #data=data.merge(data,df)\n",
    "    \n",
    "#data_df.columns=list_tickers[1:]\n",
    "data_df.columns=nondamaged_stocks\n",
    "data_df=data_df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92, 2607)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1.iloc[0:1,0:2607]\n",
    "data1 = data1.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2607, 1)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn Series(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returns(dataset):\n",
    "\treturns = list()\n",
    "\tfor i in range(1, len(dataset)):\n",
    "\t\tvalue = (dataset[i]/dataset[i - 1]) - 1\n",
    "\t\treturns.append(value)\n",
    "\treturn Series(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_difference(history, yhat, interval=1):\n",
    "\treturn yhat + history[-interval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries_to_supervised(data, lag=1):\n",
    "\tdf = pd.DataFrame(data)\n",
    "\tcolumns = [df.shift(i) for i in range(1, lag+1)]\n",
    "\tcolumns.append(df)\n",
    "\tdf = pd.concat(columns, axis=1)\n",
    "\tdf.fillna(0, inplace=True)\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries_to_supervised1(data, lag=1):\n",
    "\tdf = pd.DataFrame(data)\n",
    "\tcolumns = [df.iloc[:,1].shift(i) for i in range(1, lag+1)]\n",
    "\tcolumns.append(df)\n",
    "\tdf = pd.concat(columns, axis=1)\n",
    "\tdf.fillna(0, inplace=True)\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "\t# fit scaler\n",
    "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\tscaler = scaler.fit(train)\n",
    "\t# transform train\n",
    "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
    "\ttrain_scaled = scaler.transform(train)\n",
    "\t# transform test\n",
    "\ttest = test.reshape(test.shape[0], test.shape[1])\n",
    "\ttest_scaled = scaler.transform(test)\n",
    "\treturn scaler, train_scaled, test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "\tnew_row = [x for x in X] + [value]\n",
    "\tarray = numpy.array(new_row)\n",
    "\tarray = array.reshape(1, len(array))\n",
    "\tinverted = scaler.inverse_transform(array)\n",
    "\treturn inverted[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customloss(y_true,y_pred):    \n",
    "    from keras import backend as K\n",
    "    return K.sum(K.subtract(K.ones(K.shape(Y=y_true)[0]),K.multiply(y_true,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
    "\tX, y = train[:, 1:train.shape[1]], train[:, 0]\n",
    "\tX = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\tmodel.add(Dense(1, activation = 'tanh'))\n",
    "\tmodel.compile(loss = 'mean_squared_error', optimizer ='adam')\n",
    "\tfor i in range(nb_epoch):\n",
    "\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "\t\tmodel.reset_states()\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit an LSTM network to training data\n",
    "def fit_lstm1(train, batch_size, nb_epoch, neurons):\n",
    "\tX, y = train[:, 1:train.shape[1]], train[:, 0]\n",
    "\tX = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\tmodel.add(Dense(1, activation = 'tanh'))\n",
    "\tmodel.compile(loss = 'categorical_crossentropy', optimizer ='adam')\n",
    "\tfor i in range(nb_epoch):\n",
    "\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "\t\tmodel.reset_states()\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "\tX = X.reshape(1, 1, X.shape[0])\n",
    "\tyhat = model.predict(X, batch_size=batch_size)\n",
    "\treturn yhat[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_values = data1.values\n",
    "history = difference(data1_values, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns1 = returns(data1_values)\n",
    "returns_sign = Series([np.sign(x) if x!=0 else 1.0 for x in returns1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return_train = pd.concat(returns1,returns_sign)\n",
    "result = pd.concat([pd.DataFrame(returns1), pd.DataFrame(returns_sign)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.0241258275203]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.00359679475505]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0129746404754]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0207801513727]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0391786952134]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0436966984717]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[-0.0170300554185]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.00148148148148]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.00673898750822]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0349795918367]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[-0.0132507788776]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0105911034731]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.00846318120699]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0112156862745]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[-0.0115178779183]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.00490407626819]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[-0.0156164597486]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[-0.00281589593083]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0383406912461]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.00957597579193]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0217020146451]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0105833859408]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[-0.0278165650033]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0097138753449]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[-0.0114921015198]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0143522550839]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0114612110804]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0281991658362]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.00308719531895]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576</th>\n",
       "      <td>[0.00989836500221]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2577</th>\n",
       "      <td>[-0.00288789708585]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2578</th>\n",
       "      <td>[0.0131648235914]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2579</th>\n",
       "      <td>[-0.00459112959113]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2580</th>\n",
       "      <td>[0.0226264032721]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2581</th>\n",
       "      <td>[-0.00561654327291]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2582</th>\n",
       "      <td>[0.018998716303]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2583</th>\n",
       "      <td>[-0.00949021583942]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2584</th>\n",
       "      <td>[0.011276920468]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2585</th>\n",
       "      <td>[-0.00385679550599]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2586</th>\n",
       "      <td>[-0.0446932076425]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2587</th>\n",
       "      <td>[0.0198237885463]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588</th>\n",
       "      <td>[0.000172786177106]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2589</th>\n",
       "      <td>[0.00190031959921]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2590</th>\n",
       "      <td>[-0.0162945081473]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>[-0.0342681858019]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>[0.0382974861603]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>[-0.0187920636308]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>[-0.0160342063068]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>[0.00217273221076]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2596</th>\n",
       "      <td>[-0.0391147244806]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2597</th>\n",
       "      <td>[0.00705086020495]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2598</th>\n",
       "      <td>[0.0443427931292]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599</th>\n",
       "      <td>[0.0034861893269]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>[-0.000890789239266]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2601</th>\n",
       "      <td>[0.00953994293866]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602</th>\n",
       "      <td>[-0.00574052812859]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2603</th>\n",
       "      <td>[-0.00426363474862]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2604</th>\n",
       "      <td>[0.0150758251561]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2605</th>\n",
       "      <td>[-0.00131821776958]</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2606 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0       0\n",
       "0       [-0.0241258275203]  [-1.0]\n",
       "1       [0.00359679475505]   [1.0]\n",
       "2        [0.0129746404754]   [1.0]\n",
       "3        [0.0207801513727]   [1.0]\n",
       "4        [0.0391786952134]   [1.0]\n",
       "5        [0.0436966984717]   [1.0]\n",
       "6       [-0.0170300554185]  [-1.0]\n",
       "7       [0.00148148148148]   [1.0]\n",
       "8       [0.00673898750822]   [1.0]\n",
       "9        [0.0349795918367]   [1.0]\n",
       "10      [-0.0132507788776]  [-1.0]\n",
       "11       [0.0105911034731]   [1.0]\n",
       "12      [0.00846318120699]   [1.0]\n",
       "13       [0.0112156862745]   [1.0]\n",
       "14      [-0.0115178779183]  [-1.0]\n",
       "15      [0.00490407626819]   [1.0]\n",
       "16      [-0.0156164597486]  [-1.0]\n",
       "17     [-0.00281589593083]  [-1.0]\n",
       "18                   [0.0]       1\n",
       "19       [0.0383406912461]   [1.0]\n",
       "20      [0.00957597579193]   [1.0]\n",
       "21       [0.0217020146451]   [1.0]\n",
       "22       [0.0105833859408]   [1.0]\n",
       "23      [-0.0278165650033]  [-1.0]\n",
       "24       [0.0097138753449]   [1.0]\n",
       "25      [-0.0114921015198]  [-1.0]\n",
       "26       [0.0143522550839]   [1.0]\n",
       "27       [0.0114612110804]   [1.0]\n",
       "28       [0.0281991658362]   [1.0]\n",
       "29      [0.00308719531895]   [1.0]\n",
       "...                    ...     ...\n",
       "2576    [0.00989836500221]   [1.0]\n",
       "2577   [-0.00288789708585]  [-1.0]\n",
       "2578     [0.0131648235914]   [1.0]\n",
       "2579   [-0.00459112959113]  [-1.0]\n",
       "2580     [0.0226264032721]   [1.0]\n",
       "2581   [-0.00561654327291]  [-1.0]\n",
       "2582      [0.018998716303]   [1.0]\n",
       "2583   [-0.00949021583942]  [-1.0]\n",
       "2584      [0.011276920468]   [1.0]\n",
       "2585   [-0.00385679550599]  [-1.0]\n",
       "2586    [-0.0446932076425]  [-1.0]\n",
       "2587     [0.0198237885463]   [1.0]\n",
       "2588   [0.000172786177106]   [1.0]\n",
       "2589    [0.00190031959921]   [1.0]\n",
       "2590    [-0.0162945081473]  [-1.0]\n",
       "2591    [-0.0342681858019]  [-1.0]\n",
       "2592     [0.0382974861603]   [1.0]\n",
       "2593    [-0.0187920636308]  [-1.0]\n",
       "2594    [-0.0160342063068]  [-1.0]\n",
       "2595    [0.00217273221076]   [1.0]\n",
       "2596    [-0.0391147244806]  [-1.0]\n",
       "2597    [0.00705086020495]   [1.0]\n",
       "2598     [0.0443427931292]   [1.0]\n",
       "2599     [0.0034861893269]   [1.0]\n",
       "2600  [-0.000890789239266]  [-1.0]\n",
       "2601    [0.00953994293866]   [1.0]\n",
       "2602   [-0.00574052812859]  [-1.0]\n",
       "2603   [-0.00426363474862]  [-1.0]\n",
       "2604     [0.0150758251561]   [1.0]\n",
       "2605   [-0.00131821776958]  [-1.0]\n",
       "\n",
       "[2606 rows x 2 columns]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 2.85]), array([-0.15]))\n"
     ]
    }
   ],
   "source": [
    "print(inverse_difference(history.values, 3, 1), history.values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised1 = timeseries_to_supervised1(result, lag = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised = timeseries_to_supervised(returns1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-324-963a711562a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msupervised1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3081\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "supervised1.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = supervised1.values[0:-int(np.floor(0.2*len(supervised1)))], supervised1.values[-int(np.floor(0.2*len(supervised1))):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2085, 3)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler, train_scaled, test_scaled = scale(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2085, 1, 2)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = train[:, 1:train.shape[1]], train[:, 0]\n",
    "X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2085, 3)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are passing a target array of shape (2085, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-312-02f16aa25738>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'# fit the model\\nlstm_model = fit_lstm1(train, 1, 10, 2)\\n# forecast the entire training dataset to build up state for forecasting\\ntrain_reshaped = train[:, 1:3].reshape(len(train), 1, train.shape[1]-1)\\npredicted = lstm_model.predict(train_reshaped, batch_size=1)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-309-a0d9430540ac>\u001b[0m in \u001b[0;36mfit_lstm1\u001b[0;34m(train, batch_size, nb_epoch, neurons)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1572\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1574\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1575\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1420\u001b[0m         _check_loss_and_target_compatibility(y,\n\u001b[1;32m   1421\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_loss_fns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1422\u001b[0;31m                                              self._feed_output_shapes)\n\u001b[0m\u001b[1;32m   1423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_check_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 raise ValueError(\n\u001b[1;32m    285\u001b[0m                     \u001b[0;34m'You are passing a target array of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;34m'`categorical_crossentropy` expects '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                     \u001b[0;34m'targets to be binary matrices (1s and 0s) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are passing a target array of shape (2085, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fit the model\n",
    "# fit_lstm1(train, batch_size, nb_epoch, neurons)\n",
    "lstm_model = fit_lstm1(train, 1, 1500, 2)\n",
    "# forecast the entire training dataset to build up state for forecasting\n",
    "train_reshaped = train[:, 1:3].reshape(len(train), 1, train.shape[1]-1)\n",
    "predicted = lstm_model.predict(train_reshaped, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.],\n",
       "       [-1.],\n",
       "       [-1.],\n",
       "       ..., \n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.]], dtype=float32)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.sign(predicted)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.49160671])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = [abs(x - y)/2 for x,y in zip(pred, train[:,2])]\n",
    "(len(error) - sum(error))/len(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# walk-forward validation on the test data\n",
    "predictions = list()\n",
    "expected = list()\n",
    "for i in range(len(test)):\n",
    "\t# make one-step forecast\n",
    "\tX, y = test[i, 1:3], test[i, 0]\n",
    "\tyhat = forecast_lstm(lstm_model, 1, X)\n",
    "\t# invert scaling\n",
    "\t#yhat = invert_scale(scaler, X, yhat)\n",
    "\t# invert differencing\n",
    "\t#yhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
    "\t# store forecast\n",
    "\tpredictions.append(np.sign(yhat))\n",
    "\texpected.append(y)\n",
    "    #expected = raw_values[len(train) + i + 1]\n",
    "\t#print('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.68522073])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = [abs(x - y)/2 for x,y in zip(predictions, expected)]\n",
    "(len(error) - sum(error))/len(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.018\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD8CAYAAABkbJM/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGGZmlosThpmZ5eKEYWZmufw/zGpFYL5gfDkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1309d4750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# report performance\n",
    "rmse = np.sqrt(mean_squared_error(expected, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "# line plot of observed vs predicted\n",
    "plt.plot(expected)\n",
    "plt.plot(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'unknown'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-326-9e48d859ccb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/sklearn/linear_model/logistic.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[1;32m   1216\u001b[0m                          order=\"C\")\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/sklearn/utils/multiclass.pyc\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    170\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[1;32m    171\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'unknown'"
     ]
    }
   ],
   "source": [
    "lr = LR()\n",
    "lr.fit(train[:,1:3], train[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, array([-1.]), array([ 1.]), ..., array([ 1.]), array([-1.]),\n",
       "       array([ 1.])], dtype=object)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'unknown'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-329-7bc6adcd34a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/sklearn/linear_model/logistic.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[1;32m   1216\u001b[0m                          order=\"C\")\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python2.7/site-packages/sklearn/utils/multiclass.pyc\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    170\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[1;32m    171\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'unknown'"
     ]
    }
   ],
   "source": [
    "data = np.array(train)\n",
    "X_train = train[:, 1:3]\n",
    "y_train = train[:, 0]\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'convert_objects'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-333-e39ed8183bca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'convert_objects'"
     ]
    }
   ],
   "source": [
    "t = train.convert_objects(convert_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
